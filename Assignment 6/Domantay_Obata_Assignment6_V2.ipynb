{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Multi-layer Perceptron Classifier\n",
    "For this assignment we used the Pokemon data set from the kaggle repository\n",
    "\n",
    "https://www.kaggle.com/abcsds/pokemon\n",
    "\n",
    "Our data set contains 13 features consisting of 8 quantitative features:\n",
    "\n",
    "Encyclopedia number\n",
    "Sum of all Stats\n",
    "Hit Points\n",
    "Attack\n",
    "Defense\n",
    "Special Attack\n",
    "Special Defense\n",
    "Speed\n",
    "and 5 categorical features (sex)\n",
    "\n",
    "Name\n",
    "Type 1\n",
    "Type 2\n",
    "Generation\n",
    "Legendary\n",
    "\n",
    "For the purposes of this assignment we only used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #data analysis library\n",
    "import matplotlib.pyplot as plt #graphing\n",
    "import seaborn as sns #graphing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>Total</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>318</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>405</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>525</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>625</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #                   Name Type 1  Type 2  Total  HP  Attack  Defense  \\\n",
       "0  1              Bulbasaur  Grass  Poison    318  45      49       49   \n",
       "1  2                Ivysaur  Grass  Poison    405  60      62       63   \n",
       "2  3               Venusaur  Grass  Poison    525  80      82       83   \n",
       "3  3  VenusaurMega Venusaur  Grass  Poison    625  80     100      123   \n",
       "4  4             Charmander   Fire     NaN    309  39      52       43   \n",
       "\n",
       "   Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0       65       65     45           1      False  \n",
       "1       80       80     60           1      False  \n",
       "2      100      100     80           1      False  \n",
       "3      122      120     80           1      False  \n",
       "4       60       50     65           1      False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Pokemon.csv\") #read in data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "For the purposes of this assignment, we eliminate the following features:\n",
    "\n",
    "   * #\n",
    "   * Name\n",
    "   * Type 1\n",
    "   * Type 2\n",
    "   * Total\n",
    "   * Sp. Atk\n",
    "   * Sp. Def\n",
    "   * Generation\n",
    "   * Legendary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HP  Attack  Defense  Speed\n",
       "0  45      49       49     45\n",
       "1  60      62       63     60\n",
       "2  80      82       83     80\n",
       "3  80     100      123     80\n",
       "4  39      52       43     65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poke_stats = ['HP', \"Attack\", \"Defense\", \"Speed\"]\n",
    "df = df.drop(columns = ['#','Name', 'Type 1', 'Type 2', 'Total', 'Sp. Atk', 'Sp. Def', 'Generation', \"Legendary\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Attack  Defense  Speed\n",
       "0        49       49     45\n",
       "1        62       63     60\n",
       "2        82       83     80\n",
       "3       100      123     80\n",
       "4        52       43     65\n",
       "..      ...      ...    ...\n",
       "795     100      150     50\n",
       "796     160      110    110\n",
       "797     110       60     70\n",
       "798     160       60     80\n",
       "799     110      120     70\n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set independent and dependent variables\n",
    "x = df.iloc[:,1:9] #all entries from column 1 to 3\n",
    "leng = df.iloc[:,1]\n",
    "dim = df.iloc[:,2]\n",
    "rin = df.iloc[:,3]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      45\n",
      "1      60\n",
      "2      80\n",
      "3      80\n",
      "4      39\n",
      "       ..\n",
      "795    50\n",
      "796    50\n",
      "797    80\n",
      "798    80\n",
      "799    80\n",
      "Name: HP, Length: 800, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df.iloc[:,0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0) #20% testing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train) \n",
    "x_train = scaler.transform(x_train)\n",
    "scaler.fit(x_test)\n",
    "x_test = scaler.transform(x_test)  # apply same transformation to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \n",
      "Iterations:  1116\n",
      "Learning Rate: 'constant'\n",
      "alpha:  0.0001\n",
      "Learning Rate Iniitialization:  0.001\n",
      "Activation Function: 'relu'\n",
      "Number of Hidden Layers:  1\n",
      "Number of Neurons:  100\n"
     ]
    }
   ],
   "source": [
    "#Train and fit data\n",
    "mlp_regr = MLPRegressor(hidden_layer_sizes = (100,), random_state=1, max_iter=2000).fit(x_train,y_train)\n",
    "\n",
    "print(\"Parameters: \")\n",
    "print(\"Iterations: \", mlp_regr.n_iter_)\n",
    "print(\"Learning Rate: 'constant'\")\n",
    "print(\"alpha: \", .0001)\n",
    "print (\"Learning Rate Iniitialization: \", .001)\n",
    "print(\"Activation Function: 'relu'\")\n",
    "print(\"Number of Hidden Layers: \", mlp_regr.n_layers_ - 2)\n",
    "print(\"Number of Neurons: \", 100)\n",
    "\n",
    "#probability estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 61.07132137,  71.64285975,  81.97071186,  56.14803345,\n",
       "        73.95268867,  65.35413266,  69.09325737,  62.38211701,\n",
       "        77.65710611,  74.53535991,  52.61166673,  70.44264011,\n",
       "        86.95545631,  75.94001198,  73.36875251,  81.88829366,\n",
       "        81.53668183,  64.98708538,  59.98862375,  69.54599656,\n",
       "        49.17337536,  57.46090933,  55.67214031,  61.42939942,\n",
       "        61.28662387, 106.75482771,  66.1824447 ,  81.55876971,\n",
       "        84.90250358,  57.88981736,  80.10813134,  49.26756228,\n",
       "        80.56843862,  65.73080056,  52.50592405,  63.33500428,\n",
       "        93.37060748,  60.516072  ,  54.12451554,  68.00640133,\n",
       "        63.68065213,  59.38490053,  83.02051844,  88.78720186,\n",
       "        80.75713722,  63.663905  ,  55.08961133,  89.37684281,\n",
       "        72.80923601,  64.13275861,  63.961776  ,  48.86099241,\n",
       "        90.57438351,  59.00854246,  50.73432709,  58.61348553,\n",
       "        71.35591895,  46.31785626,  53.10675995,  73.04026902,\n",
       "        62.20295264,  86.02566625,  53.58691298,  58.9932116 ,\n",
       "        61.926405  ,  64.55243057,  84.65558801,  53.33623322,\n",
       "        90.82473238,  54.36835766,  64.04456051,  80.114907  ,\n",
       "        51.34341892,  61.10389687,  54.93845171,  87.86138523,\n",
       "        62.76064919,  71.90615635,  56.78893277,  47.74300533,\n",
       "        81.66004225, 102.15669873,  86.99088572,  77.17926493,\n",
       "        66.42334757,  49.89475442,  64.82044007,  62.81650887,\n",
       "        52.58582701,  54.86635249,  76.05128237,  46.92366003,\n",
       "        56.27617035,  81.12583183,  74.57178792,  73.65784877,\n",
       "        70.8568229 ,  75.32944655,  62.81650887,  77.15754485,\n",
       "        88.78720186,  51.27510701,  66.52046636,  84.9404885 ,\n",
       "        91.25164337,  49.14055192,  86.13863699,  82.02438206,\n",
       "        70.73665653,  51.34341892,  85.19393177,  48.5308928 ,\n",
       "        67.29306431,  59.9911478 ,  79.26641811,  96.8413988 ,\n",
       "        91.13778285,  71.71907752,  57.14308727,  67.9981638 ,\n",
       "        78.06657014,  88.78720186,  68.99053741,  56.06782591,\n",
       "        48.40587492,  92.81270184,  74.65934828,  71.90166055,\n",
       "        68.12646868,  65.10421925,  58.31503046,  88.9298827 ,\n",
       "        52.44123002,  69.91409323,  59.39575044,  77.38675223,\n",
       "        82.44497184,  86.8676087 ,  52.95267534,  62.55335505,\n",
       "        58.44069763,  61.55636754,  61.60063646,  58.4441572 ,\n",
       "        71.79738533,  51.2505353 ,  73.3382621 ,  47.50891549,\n",
       "        52.69121479,  63.89097965,  64.41956874,  50.17547624,\n",
       "        59.29697005,  81.81951712,  54.25105736,  84.64250427,\n",
       "        80.30461403,  63.9845605 ,  74.75483866,  54.55870953,\n",
       "        94.74841956,  55.88497843,  55.78720185,  63.61122031,\n",
       "        59.38490053,  61.48291544,  84.17945563,  87.34236334,\n",
       "        76.45794185,  49.85459611,  68.60520936,  62.94514952,\n",
       "        55.36425554,  51.86766655,  50.58069435,  77.38675223,\n",
       "        60.07762961,  87.39316324,  88.00931211,  46.80963297,\n",
       "        73.04026902,  63.40404486,  76.83483509,  65.99371261,\n",
       "        68.53980513,  72.01972062,  80.28339817,  49.25197746,\n",
       "        58.6892295 ,  79.68008379,  52.23386449,  68.55806907,\n",
       "        48.68183156,  54.62997801,  51.2505353 ,  53.35888187,\n",
       "        54.17833575,  70.91830136,  67.06053809,  65.63456281,\n",
       "        78.05322349,  48.20344858,  79.16346143,  87.20293189,\n",
       "        53.88331593,  85.84404134,  81.17181307,  65.24033615,\n",
       "        66.23528785,  64.93391932,  82.24588395,  50.24432854,\n",
       "        50.4999854 ,  70.13878459,  69.27309651,  69.71791398,\n",
       "        56.03752183,  55.94609011,  72.95189215,  63.22787778,\n",
       "        52.13456025,  61.03336786,  61.24722664,  70.42273502,\n",
       "        75.07861848,  91.54970141,  50.56274277,  72.90062504,\n",
       "        67.710564  ,  61.06605853,  69.52663653,  78.37861024,\n",
       "        62.93492613,  81.48854831,  52.20737123,  53.05347059,\n",
       "        80.78792148,  74.79707828,  49.51159097,  49.55779231,\n",
       "        75.57852526,  74.98821567,  55.02287294,  52.05008075,\n",
       "        63.23478365,  63.94833544,  69.17692707,  72.28659242,\n",
       "        74.25572116,  73.43912152,  99.65499795,  71.09048722,\n",
       "        94.32218475,  64.97330512,  55.37893362,  53.2772908 ,\n",
       "        55.81734681,  81.32504241,  55.65634654,  71.86824243,\n",
       "        76.64100961,  60.22567667,  88.78720186,  79.1787751 ,\n",
       "        56.98795784,  68.53447583,  64.36230917,  50.79737303,\n",
       "        71.29190003,  88.02117923,  51.79511327,  76.41547645,\n",
       "        85.15899899,  75.52646981,  51.789285  ,  56.94323683,\n",
       "        59.12790713,  76.55105191,  80.27879451,  66.00985897,\n",
       "        73.27772116,  58.13398426,  75.16434656,  68.96662199,\n",
       "        84.25093607,  55.51321389,  59.25480263,  76.82942454,\n",
       "        78.0262748 ,  66.53980481,  66.56392531,  54.54157205,\n",
       "        84.24972304,  74.26688313,  63.74354092,  73.04026902,\n",
       "        72.4402102 ,  69.36951565,  60.16855275,  99.37598063,\n",
       "        91.10963313,  82.22161706,  82.57964927,  52.42256809,\n",
       "        65.54091248,  52.93987955,  66.91822412,  75.44444159,\n",
       "        68.8590989 ,  56.54704195,  55.74954984,  73.04026902,\n",
       "        56.10804952,  88.78720186,  74.22285926,  99.88014765,\n",
       "        86.70576676,  59.31172262,  58.21593857,  53.29845919,\n",
       "        79.27637458,  80.02303508,  54.99106322,  62.82700106,\n",
       "        55.1620362 ,  62.48115045,  61.07985845,  89.66209823,\n",
       "        84.68515367,  58.45773392,  62.95932524,  56.68315652,\n",
       "        52.65017203,  64.23799683,  74.54905192,  68.55615796,\n",
       "        66.62926278,  87.20779397,  74.79707828,  64.13275861,\n",
       "        95.92317407,  75.9673415 ,  52.58382324,  69.52771834,\n",
       "        89.50794035,  70.03017952,  78.22386951,  75.27758725,\n",
       "        61.81391426,  65.21590938,  53.72028412,  56.29096363,\n",
       "        70.29872918,  97.54203577,  73.9828985 ,  85.36589666,\n",
       "        62.51562444,  54.7966253 ,  66.51881099,  48.19497913,\n",
       "        46.43130267,  75.71260414,  86.07072298,  84.37845433,\n",
       "        70.91830136,  52.11590645,  71.30288899,  53.0768218 ,\n",
       "        76.42023299,  81.70674301,  54.21352485,  49.38885867,\n",
       "        75.95751452,  67.64691151,  87.72271323,  56.10804952,\n",
       "        51.86614032,  83.14824979,  51.16878618,  72.30462854,\n",
       "        67.17307555,  43.80573366,  49.1809439 ,  71.23658366,\n",
       "        85.30317734,  60.354418  ,  51.17386444,  60.59342115,\n",
       "        58.27943444,  40.40844949,  74.05589062,  55.78504915,\n",
       "        49.30205234,  65.41218128,  88.6827454 ,  62.52545917,\n",
       "        59.03829856,  78.13252087,  71.42426806,  51.50493643,\n",
       "        58.67260392,  66.02491121,  70.66733802,  78.43689205,\n",
       "        51.86254417,  72.76852665,  55.377827  ,  64.36105141,\n",
       "        60.82918279,  69.42769832,  55.3187617 ,  90.36474768,\n",
       "        58.59447503,  90.36732383,  49.02523389,  90.57438351,\n",
       "        63.39037788,  57.90950378,  53.19730463, 102.94054885,\n",
       "        79.30516581,  49.27976367,  62.94081612,  51.82672633,\n",
       "        57.73341174,  72.40416332,  57.96193336,  53.2219352 ,\n",
       "        78.42340049,  58.0839301 ,  58.4441572 ,  80.44703033,\n",
       "        73.57865554,  79.4803944 ,  70.46742205,  90.61391961,\n",
       "        82.09006777,  57.48139332,  57.29091072,  53.79585071,\n",
       "        75.40453918,  52.56992753,  86.70576676,  86.2693084 ,\n",
       "        68.30183952,  73.60943666,  85.49926931,  92.56942925,\n",
       "        54.19572045,  64.37448939,  81.42267725,  49.47353369,\n",
       "        46.97808496,  61.14755948,  56.05240938,  78.6572132 ,\n",
       "        80.26433633,  53.8106291 ,  62.0161975 ,  53.02435441,\n",
       "        96.36951501,  67.21432541,  69.00112414,  61.5769589 ,\n",
       "        79.52247258,  75.5861547 ,  65.54340393,  54.89974774,\n",
       "        69.93704932,  65.09296602,  75.05247633,  87.80312842,\n",
       "        50.8661885 ,  73.97260147,  60.18537298,  62.77287067,\n",
       "        68.04546551,  50.35889919,  48.75259687,  86.40068701,\n",
       "        55.99535664,  73.86596712,  71.30235797,  80.07639717,\n",
       "        46.10105472,  83.07977409,  49.7724723 ,  58.5201794 ,\n",
       "        81.73438589,  88.41178368,  54.36826536,  76.05247806,\n",
       "        59.94577517,  74.42827244,  55.49433421,  88.34079707,\n",
       "        87.62609968,  72.88708053,  87.83839132,  64.71128423,\n",
       "        59.56891906,  50.05403149,  76.17159228,  66.98087432,\n",
       "        56.47278869,  48.2196039 ,  49.9634079 ,  87.62442883,\n",
       "        58.81802586,  73.30666252,  66.91822412,  74.25572116,\n",
       "        66.69055424,  63.43281576,  51.13194414,  78.35167801,\n",
       "        75.13850411,  63.28182822,  83.11321443,  63.65160179,\n",
       "        69.16874517,  69.19607351,  84.62240019,  60.680332  ,\n",
       "        59.25398741,  89.97149552,  81.65026217,  54.36799806,\n",
       "        73.23326268,  51.5254022 ,  77.28554383,  74.36154282,\n",
       "        86.08269847,  68.94971392,  83.22364132,  71.07612612,\n",
       "        70.13878459,  74.04228133,  79.0307409 ,  52.16922978,\n",
       "        71.90649735,  89.72224731,  59.25480263,  62.13203613,\n",
       "        81.42267725,  47.07737742,  71.71907752,  62.83932476,\n",
       "        72.01535096,  62.49553847,  52.65017203,  48.76515468,\n",
       "        68.05100695,  52.68607629,  67.47493426,  80.77925154,\n",
       "        54.77524853,  51.07270646,  51.09013001,  60.47954199,\n",
       "        80.70977449,  75.5861547 ,  50.60134683,  53.81453026,\n",
       "        74.10812794,  50.32501221,  55.5368612 ,  55.58931497,\n",
       "        56.94220051,  64.11850606,  80.7787298 ,  71.65000021,\n",
       "        67.30334879,  59.03829856,  45.82579221,  86.48551096,\n",
       "        50.4172797 ,  82.33446036,  63.42966064,  48.19617928,\n",
       "        75.45828373,  46.96537589,  68.397467  ,  55.51321389,\n",
       "        69.51247259,  88.78720186,  53.11466155,  65.43491367,\n",
       "        79.33990928,  55.28490809,  74.50702529,  66.94364177,\n",
       "        80.60576042,  85.04959352,  73.45351119,  55.17479656,\n",
       "        80.28339817,  97.41245249,  84.94616779,  55.49433421,\n",
       "        71.57675887,  56.82256682,  60.44672331,  96.23027002,\n",
       "        82.60163564,  68.87373433,  59.48890627,  54.96539396,\n",
       "        42.3623054 ,  54.86036464,  79.03289968,  90.69587466,\n",
       "        78.72081771,  85.73029456,  63.73861285,  76.86199489,\n",
       "        48.27078133,  82.80371863,  84.57318362,  60.43415764,\n",
       "        73.76479559,  61.42903549,  75.33077274,  52.37170218,\n",
       "        79.4803944 ,  56.22276782,  47.64750694,  51.58471265,\n",
       "        83.0866653 ,  54.2244669 ,  72.07836545,  71.64704115,\n",
       "        53.03437871,  71.00922318,  96.6685243 ,  71.49504342,\n",
       "        73.2604243 ,  56.9996568 ,  84.41588987,  88.33223758])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = mlp_regr.predict(x_train)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.3168650996013702\n",
      "R:  0.5629077185484049\n"
     ]
    }
   ],
   "source": [
    "mlp_score = mlp_regr.score(x_train,y_train)\n",
    "mlp_score\n",
    "print(\"R^2: \", mlp_score)\n",
    "print(\"R: \", np.sqrt(mlp_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Training Set: \n",
      "Absolute Error:  13.403091834095338\n",
      "MSE:  413.05694213164514\n",
      "RMSE:  20.323802354176866\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation Metrics for Training Set: \")\n",
    "print(\"Absolute Error: \" ,metrics.mean_absolute_error(y_train, ypred)) #Absolute error\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_train, ypred)) #MSE\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_train,ypred))) #RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44.37548176,  79.24652554,  60.25472595,  54.53446779,\n",
       "        67.82449957,  62.63304518,  60.39731093,  79.55705269,\n",
       "        57.74620896,  80.69905152,  56.22241727,  53.3603741 ,\n",
       "        62.54083885,  86.48575157,  68.80210686,  41.58373189,\n",
       "        54.16440128,  76.42277003,  79.36276169,  74.85102389,\n",
       "        47.77352413,  68.10236811,  86.07899523,  88.43884541,\n",
       "        78.13597086,  49.2815806 ,  80.59462431,  61.53978718,\n",
       "        76.57755845,  80.5958376 ,  77.82340389,  60.63366754,\n",
       "        59.47565506,  68.21430688,  87.70259768,  62.07121319,\n",
       "        79.49953797,  71.83510883,  73.1397742 ,  53.66347419,\n",
       "        52.57417464,  95.8438388 ,  70.18781025,  68.48084226,\n",
       "        53.66347419,  56.92408449,  58.98615203,  66.877922  ,\n",
       "        57.26871751,  66.04829868,  64.93735025,  51.5844842 ,\n",
       "        49.16932199,  56.9960055 ,  58.3898752 ,  51.83885739,\n",
       "        92.70260779,  80.45182231,  61.60175076,  76.8409267 ,\n",
       "        86.96283534,  76.74268519,  68.60272671,  56.97632041,\n",
       "        61.07759776,  32.65150656,  41.61100354,  75.73148777,\n",
       "        62.70475839,  89.65489172,  72.19253163,  70.82772419,\n",
       "        59.37420566,  67.44420431,  61.11498804,  64.42008068,\n",
       "        72.11075988,  74.46472962,  52.56108738,  77.73705924,\n",
       "        82.28708726,  54.3362613 ,  64.9677187 ,  73.88934603,\n",
       "        82.08291642,  73.01542423,  67.3394895 ,  67.43244455,\n",
       "        61.62385514,  72.45209689,  51.6656621 ,  60.83319968,\n",
       "        87.47171108,  67.9099781 ,  71.44610321, 102.29749946,\n",
       "        66.27655138,  84.25657942,  77.08948301,  73.59417809,\n",
       "        84.39527216,  81.37665046,  70.30227759,  67.03903686,\n",
       "        60.08471297,  59.83057602,  87.70300761,  87.05984658,\n",
       "        87.71214627,  52.83442863,  94.2802079 ,  70.56524178,\n",
       "        79.06761835,  52.5592448 ,  72.76413897,  80.19195271,\n",
       "        79.2140643 ,  79.71053015,  73.10913968,  68.4259654 ,\n",
       "        75.62586301,  61.11282976,  63.8005438 ,  74.33793144,\n",
       "        40.87027139,  84.91153287,  81.57372634,  64.32444383,\n",
       "        64.84260086,  55.47516507,  51.5844842 ,  55.78132136,\n",
       "        78.73178785,  88.2993162 ,  88.2993162 ,  61.58933236,\n",
       "        74.93257049,  83.37073765,  65.06401513,  63.35630064,\n",
       "        48.17479705,  57.33103085,  50.0339057 ,  81.94118663,\n",
       "        81.35093147,  91.12959981,  84.48593393,  62.3958992 ,\n",
       "        58.93032979,  74.52244888,  68.05787801,  53.06875776,\n",
       "        51.50651004,  57.94243567,  55.59166077,  55.73101551,\n",
       "        48.28371691,  87.69898798,  70.93111749,  51.8947304 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = mlp_regr.predict(x_test)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.1646513181266498\n",
      "R:  0.40577249552754285\n"
     ]
    }
   ],
   "source": [
    "mlp_test_score = mlp_regr.score(x_test,y_test)\n",
    "print(\"R^2: \", mlp_test_score)\n",
    "print(\"R: \", np.sqrt(mlp_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Test Set: \n",
      "Absolute Error:  16.412070434778144\n",
      "MSE:  682.2767789422467\n",
      "RMSE:  26.120428383589857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluation Metrics for Test Set: \")\n",
    "print(\"Absolute Error: \" ,metrics.mean_absolute_error(y_test, y_test_pred)) #Absolute error\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_test, y_test_pred)) #MSE\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test,y_test_pred))) #RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Regressor Alternate Parameters #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \n",
      "Iterations:  323\n",
      "Learning Rate: 'adaptive'\n",
      "alpha:  0.001\n",
      "Learning Rate Iniitialization:  0.001\n",
      "Activation Function: 'relu'\n",
      "Number of Hidden Layers:  1\n",
      "Number of Neurons:  5000\n",
      "\n",
      "Training Set Performance Metrics\n",
      "R^2:  0.3210706906730604\n",
      "R:  0.5666310004518464\n",
      "\n",
      "Evaluation Metrics for Training Set: \n",
      "Absolute Error:  13.230800223254999\n",
      "MSE:  410.5140350324547\n",
      "RMSE:  20.26114594568764\n",
      "\n",
      "Test Set Performance Metrics\n",
      "R^2:  0.15720150009130773\n",
      "R:  0.39648644376738496\n",
      "\n",
      "Evaluation Metrics for Test Set: \n",
      "Absolute Error:  16.334191039477908\n",
      "MSE:  688.3614690401113\n",
      "RMSE:  26.236643631381497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train and fit data\n",
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(5000,), activation='relu',\n",
    "                        alpha=0.001, learning_rate='adaptive', learning_rate_init=0.001,\n",
    "                        max_iter=2000, random_state=1, tol=0.0001).fit(x_train,y_train)\n",
    "\n",
    "print(\"Parameters: \")\n",
    "print(\"Iterations: \", mlp_regr.n_iter_)\n",
    "print(\"Learning Rate: 'adaptive'\")\n",
    "print(\"alpha: \", .001)\n",
    "print (\"Learning Rate Iniitialization: \", .001)\n",
    "print(\"Activation Function: 'relu'\")\n",
    "print(\"Number of Hidden Layers: \", mlp_regr.n_layers_ - 2)\n",
    "print(\"Number of Neurons: \", 5000)\n",
    "\n",
    "print(\"\\nTraining Set Performance Metrics\")\n",
    "\n",
    "ypred = mlp_regr.predict(x_train)\n",
    "mlp_score = mlp_regr.score(x_train,y_train)\n",
    "print(\"R^2: \", mlp_score)\n",
    "print(\"R: \", np.sqrt(mlp_score))\n",
    "print(\"\\nEvaluation Metrics for Training Set: \")\n",
    "print(\"Absolute Error: \" ,metrics.mean_absolute_error(y_train, ypred)) #Absolute error\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_train, ypred)) #MSE\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_train,ypred))) #RMSE\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Performance Metrics\")\n",
    "\n",
    "y_test_pred = mlp_regr.predict(x_test)\n",
    "mlp_test_score = mlp_regr.score(x_test,y_test)\n",
    "print(\"R^2: \", mlp_test_score)\n",
    "print(\"R: \", np.sqrt(mlp_test_score))\n",
    "print(\"\\nEvaluation Metrics for Test Set: \")\n",
    "print(\"Absolute Error: \" ,metrics.mean_absolute_error(y_test, y_test_pred)) #Absolute error\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_test, y_test_pred)) #MSE\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test,y_test_pred))) #RMSE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Regressor Alternate Parameters #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \n",
      "Iterations:  77\n",
      "Learning Rate: 'adaptive'\n",
      "alpha:  0.0001\n",
      "Learning Rate Iniitialization:  0.001\n",
      "Activation Function: 'identity'\n",
      "Number of Hidden Layers:  2\n",
      "Number of Neurons:  100\n",
      "\n",
      "Training Set Performance Metrics\n",
      "R^2:  0.20462065467304746\n",
      "R:  0.4523501460959723\n",
      "\n",
      "Evaluation Metrics for Training Set: \n",
      "Absolute Error:  15.021714646255697\n",
      "MSE:  480.9254512157257\n",
      "RMSE:  21.930012567614405\n",
      "\n",
      "Test Set Performance Metrics\n",
      "R^2:  0.07386298004198288\n",
      "R:  0.2717774457933971\n",
      "\n",
      "Evaluation Metrics for Test Set: \n",
      "Absolute Error:  17.47737947648736\n",
      "MSE:  756.4287782427227\n",
      "RMSE:  27.50325032142061\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Train and fit data\n",
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(250,5), activation='identity', \n",
    "                        alpha=0.0001, learning_rate='adaptive', learning_rate_init=0.001,\n",
    "                        max_iter=2000, random_state=1).fit(x_train,y_train)\n",
    "\n",
    "print(\"Parameters: \")\n",
    "print(\"Iterations: \", mlp_regr.n_iter_)\n",
    "print(\"Learning Rate: 'adaptive'\")\n",
    "print(\"alpha: \", .0001)\n",
    "print (\"Learning Rate Iniitialization: \", .001)\n",
    "print(\"Activation Function: 'identity'\")\n",
    "print(\"Number of Hidden Layers: \", mlp_regr.n_layers_ - 2)\n",
    "print(\"Number of Neurons: \", 250)\n",
    "\n",
    "print(\"\\nTraining Set Performance Metrics\")\n",
    "\n",
    "ypred = mlp_regr.predict(x_train)\n",
    "mlp_score = mlp_regr.score(x_train,y_train)\n",
    "print(\"R^2: \", mlp_score)\n",
    "print(\"R: \", np.sqrt(mlp_score))\n",
    "print(\"\\nEvaluation Metrics for Training Set: \")\n",
    "print(\"Absolute Error: \" ,metrics.mean_absolute_error(y_train, ypred)) #Absolute error\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_train, ypred)) #MSE\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_train,ypred))) #RMSE\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Performance Metrics\")\n",
    "\n",
    "y_test_pred = mlp_regr.predict(x_test)\n",
    "mlp_test_score = mlp_regr.score(x_test,y_test)\n",
    "print(\"R^2: \", mlp_test_score)\n",
    "print(\"R: \", np.sqrt(mlp_test_score))\n",
    "print(\"\\nEvaluation Metrics for Test Set: \")\n",
    "print(\"Absolute Error: \" ,metrics.mean_absolute_error(y_test, y_test_pred)) #Absolute error\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_test, y_test_pred)) #MSE\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test,y_test_pred))) #RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier Alternate Parameters # 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \n",
      "Iterations:  181\n",
      "Learning Rate: 'constant'\n",
      "alpha:  0.01\n",
      "Learning Rate Iniitialization:  0.01\n",
      "Activation Function: 'relu'\n",
      "Number of Hidden Layers:  2\n",
      "Number of Neurons:  50\n",
      "\n",
      "Training Set Performance Metrics\n",
      "R^2:  0.3171375028780288\n",
      "R:  0.5631496274330906\n",
      "\n",
      "Evaluation Metrics for Training Set: \n",
      "Absolute Error:  13.278759177872228\n",
      "MSE:  412.8922337198548\n",
      "RMSE:  20.319749843929053\n",
      "\n",
      "Test Set Performance Metrics\n",
      "R^2:  0.1517552773027726\n",
      "R:  0.3895577971274258\n",
      "\n",
      "Evaluation Metrics for Test Set: \n",
      "Absolute Error:  16.791522963192403\n",
      "MSE:  692.8097089454291\n",
      "RMSE:  26.321278634318453\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train and fit data\n",
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(50,5), activation='relu', \n",
    "                        alpha=0.01, learning_rate='constant', learning_rate_init=0.01,\n",
    "                        max_iter=5000, random_state=1, tol=0.0001).fit(x_train,y_train)\n",
    "\n",
    "print(\"Parameters: \")\n",
    "print(\"Iterations: \", mlp_regr.n_iter_)\n",
    "print(\"Learning Rate: 'constant'\")\n",
    "print(\"alpha: \", .01)\n",
    "print (\"Learning Rate Iniitialization: \", .01)\n",
    "print(\"Activation Function: 'relu'\")\n",
    "print(\"Number of Hidden Layers: \", mlp_regr.n_layers_ - 2)\n",
    "print(\"Number of Neurons: \", 50)\n",
    "\n",
    "print(\"\\nTraining Set Performance Metrics\")\n",
    "\n",
    "ypred = mlp_regr.predict(x_train)\n",
    "mlp_score = mlp_regr.score(x_train,y_train)\n",
    "print(\"R^2: \", mlp_score)\n",
    "print(\"R: \", np.sqrt(mlp_score))\n",
    "print(\"\\nEvaluation Metrics for Training Set: \")\n",
    "print(\"Absolute Error: \" ,metrics.mean_absolute_error(y_train, ypred)) #Absolute error\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_train, ypred)) #MSE\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_train,ypred))) #RMSE\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Performance Metrics\")\n",
    "\n",
    "y_test_pred = mlp_regr.predict(x_test)\n",
    "mlp_test_score = mlp_regr.score(x_test,y_test)\n",
    "print(\"R^2: \", mlp_test_score)\n",
    "print(\"R: \", np.sqrt(mlp_test_score))\n",
    "print(\"\\nEvaluation Metrics for Test Set: \")\n",
    "print(\"Absolute Error: \" ,metrics.mean_absolute_error(y_test, y_test_pred)) #Absolute error\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_test, y_test_pred)) #MSE\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test,y_test_pred))) #RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "    At one point, we compared the performance of two solvers for weight optimization. The 'adam\" solver, porposed by Kingma, Iederik, and Ba, performed well. The \"lbfgs' solver, which is better suited for small data sets seemed to take longer to converge which seems to indicate that the data set we are working with is considered a \"large dataset.\" \n",
    "    We experimented with lowering the number of neurons and found that decreasing the number of neurons caused iteration count to increase. While increasing the number of neurons decreased iteraction count. When increasing neuron count, evaluation metrics improved slightly, and when decreasing neuron count, evaluation metrics worsened slightly. \n",
    "    Changing the alpha and learning rate did not seem to have much of an impact, especially in comparison to altering the number of neurons. However, we did note that lowering the alpha and learning rate initialization seemed to increase iteration count dramatically. \n",
    "    Decreasing the number of hidden layers also appeared to lower the iteration count and slightly improve evaluation metrics (< 1 point). \n",
    "    Note that all these observations were consistent for both training and test sets. \n",
    "    In comparison to the 'relu' activation function, the 'identity' activation function converged extremely quickly, but had slightly worse evaluation metrics (higher error scores and lower R scores). \n",
    "    *Note that above examples do not necessarily reflect these findings, as we altered each parameter individually for a control regressor and noted the changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison to Assignment 2:\n",
    "    Our best regressor utilized mostly default parameters, with the exception of an adaptive learning rate, and a high number of neurons and hidden layers. \n",
    "    Using our best performing results: in comparison to the OLS algorithm from assignment 2, we noted that our evaluation metrics improved significantly for the training set, and slightly for the test set. In particular, the R^2 score was slightly better, but the OLS algorithm had better error scores. \n",
    "    In comparison to Gradient Descent: Our Regressor had slightly higher errors for both the training and test set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
